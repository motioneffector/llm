<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Sending Messages - @motioneffector/llm</title>
  <link rel="stylesheet" href="../demo-files/demo.css">
  <link rel="stylesheet" href="manual.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>document.addEventListener('DOMContentLoaded', () => hljs.highlightAll());</script>
</head>
<body>
  <div class="page">
    <header class="header">
      <h1 class="header-title">@motioneffector/llm</h1>
      <p class="header-description">Documentation</p>
      <nav class="header-links">
        <a href="../index.html" class="header-link">Demo</a>
        <a href="https://www.npmjs.com/package/@motioneffector/llm" class="header-link">npm</a>
        <a href="https://github.com/motioneffector/llm" class="header-link">GitHub</a>
      </nav>
    </header>

    <div class="manual-layout">
      <aside class="manual-sidebar">
        <nav class="sidebar-nav">
<p><strong><a href="index.html">@motioneffector/llm</a></strong></p>
<p><strong>Getting Started</strong></p>
<ul>
<li><a href="installation.html">Installation</a></li>
<li><a href="your-first-chat.html">Your First Chat</a></li>
</ul>
<p><strong>Core Concepts</strong></p>
<ul>
<li><a href="concept-client.html">Client</a></li>
<li><a href="concept-messages.html">Messages</a></li>
<li><a href="concept-streaming.html">Streaming</a></li>
<li><a href="concept-conversations.html">Conversations</a></li>
<li><a href="concept-error-handling.html">Error Handling</a></li>
</ul>
<p><strong>Guides</strong></p>
<ul>
<li><a href="guide-sending-messages.html">Sending Messages</a></li>
<li><a href="guide-streaming-responses.html">Streaming Responses</a></li>
<li><a href="guide-building-conversations.html">Building Conversations</a></li>
<li><a href="guide-error-handling.html">Error Handling</a></li>
<li><a href="guide-canceling-requests.html">Canceling Requests</a></li>
<li><a href="guide-using-different-providers.html">Using Different Providers</a></li>
</ul>
<p><strong>API Reference</strong></p>
<ul>
<li><a href="api-client.html">Client API</a></li>
<li><a href="api-conversation.html">Conversation API</a></li>
<li><a href="api-types.html">Types</a></li>
<li><a href="api-errors.html">Errors</a></li>
<li><a href="api-utilities.html">Utilities</a></li>
</ul>

        </nav>
      </aside>

      <main class="manual-content">
        <article class="manual-article">
<h1>Sending Messages</h1>
<p>Send chat messages to an LLM and process the response. This guide covers message formatting, request options, and handling the response object.</p>
<h2>Prerequisites</h2>
<p>Before starting, you should:</p>
<ul>
<li><a href="installation.html">Install the package</a></li>
<li>Have an OpenRouter API key</li>
</ul>
<h2>Overview</h2>
<p>We&#39;ll send a chat message by:</p>
<ol>
<li>Creating a client with credentials</li>
<li>Building a messages array</li>
<li>Calling <code>chat()</code> with options</li>
<li>Processing the response</li>
</ol>
<h2>Step 1: Create the Client</h2>
<p>The client needs your API key and a default model. Create it once and reuse it for all requests.</p>
<pre><code class="language-typescript">import { createLLMClient } from &#39;@motioneffector/llm&#39;

const client = createLLMClient({
  apiKey: process.env.OPENROUTER_KEY!,
  model: &#39;anthropic/claude-sonnet-4&#39;
})
</code></pre>
<h2>Step 2: Build the Messages Array</h2>
<p>Messages are objects with <code>role</code> and <code>content</code>. The role is <code>system</code>, <code>user</code>, or <code>assistant</code>.</p>
<pre><code class="language-typescript">const messages = [
  { role: &#39;system&#39; as const, content: &#39;You are a helpful assistant.&#39; },
  { role: &#39;user&#39; as const, content: &#39;What is TypeScript?&#39; }
]
</code></pre>
<p>For simple requests, you can skip the system message:</p>
<pre><code class="language-typescript">const messages = [
  { role: &#39;user&#39; as const, content: &#39;What is TypeScript?&#39; }
]
</code></pre>
<h2>Step 3: Send the Request</h2>
<p>Call <code>chat()</code> with your messages. The method returns a promise that resolves to the response.</p>
<pre><code class="language-typescript">const response = await client.chat(messages)
</code></pre>
<h2>Step 4: Process the Response</h2>
<p>The response object contains the generated text and metadata:</p>
<pre><code class="language-typescript">console.log(response.content)        // The generated text
console.log(response.usage)          // Token counts
console.log(response.model)          // Model that responded
console.log(response.latency)        // Request time in ms
console.log(response.finishReason)   // Why generation stopped
</code></pre>
<p>The <code>usage</code> object has detailed token counts:</p>
<pre><code class="language-typescript">console.log(response.usage.promptTokens)      // Input tokens
console.log(response.usage.completionTokens)  // Output tokens
console.log(response.usage.totalTokens)       // Total
</code></pre>
<h2>Complete Example</h2>
<pre><code class="language-typescript">import { createLLMClient } from &#39;@motioneffector/llm&#39;

const client = createLLMClient({
  apiKey: process.env.OPENROUTER_KEY!,
  model: &#39;anthropic/claude-sonnet-4&#39;
})

const response = await client.chat([
  { role: &#39;system&#39;, content: &#39;Be concise.&#39; },
  { role: &#39;user&#39;, content: &#39;Explain async/await in JavaScript.&#39; }
])

console.log(response.content)
console.log(`Tokens: ${response.usage.totalTokens}, Latency: ${response.latency}ms`)
</code></pre>
<h2>Variations</h2>
<h3>With Temperature</h3>
<p>Control randomness with temperature (0 = deterministic, 2 = very random):</p>
<pre><code class="language-typescript">const response = await client.chat(messages, {
  temperature: 0.7
})
</code></pre>
<h3>With Max Tokens</h3>
<p>Limit response length:</p>
<pre><code class="language-typescript">const response = await client.chat(messages, {
  maxTokens: 500
})
</code></pre>
<h3>With Stop Sequences</h3>
<p>Stop generation when specific text appears:</p>
<pre><code class="language-typescript">const response = await client.chat(messages, {
  stop: [&#39;END&#39;, &#39;---&#39;]
})
</code></pre>
<h3>Override Model Per-Request</h3>
<p>Use a different model for one request:</p>
<pre><code class="language-typescript">const response = await client.chat(messages, {
  model: &#39;openai/gpt-4o&#39;
})
</code></pre>
<h3>Combining Options</h3>
<p>Options can be combined:</p>
<pre><code class="language-typescript">const response = await client.chat(messages, {
  temperature: 0.3,
  maxTokens: 1000,
  model: &#39;openai/gpt-4o&#39;
})
</code></pre>
<h2>Troubleshooting</h2>
<h3>Empty Response</h3>
<p><strong>Symptom:</strong> <code>response.content</code> is empty.</p>
<p><strong>Cause:</strong> Model hit max tokens or content filter.</p>
<p><strong>Solution:</strong> Check <code>response.finishReason</code>. If <code>&#39;length&#39;</code>, increase <code>maxTokens</code>. If <code>&#39;content_filter&#39;</code>, the response was blocked.</p>
<h3>Rate Limit Errors</h3>
<p><strong>Symptom:</strong> <code>RateLimitError</code> thrown.</p>
<p><strong>Cause:</strong> Too many requests to the API.</p>
<p><strong>Solution:</strong> The library retries automatically. For persistent issues, add delays between requests or check your API quota.</p>
<h3>Slow Responses</h3>
<p><strong>Symptom:</strong> Requests take several seconds.</p>
<p><strong>Cause:</strong> Large prompts or model processing time.</p>
<p><strong>Solution:</strong> Check <code>response.usage.promptTokens</code>. Reduce context if too large. Consider using <a href="guide-streaming-responses.html">streaming</a> for perceived performance.</p>
<h2>See Also</h2>
<ul>
<li><strong><a href="guide-streaming-responses.html">Streaming Responses</a></strong> - Real-time output</li>
<li><strong><a href="concept-messages.html">Messages</a></strong> - Message format details</li>
<li><strong><a href="api-client.html">Client API</a></strong> - Full <code>chat()</code> reference</li>
</ul>

        </article>
      </main>
    </div>
  </div>
</body>
</html>
