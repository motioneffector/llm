# @motioneffector/llm - Test Specification

Test-driven development specification for the LLM client library.

---

## Type Definitions

Reference types used throughout tests:

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant'
  content: string
}

interface TokenUsage {
  promptTokens: number
  completionTokens: number
  totalTokens: number
}

interface ChatResponse {
  content: string
  usage: TokenUsage
  model: string
  id: string
  finishReason: 'stop' | 'length' | 'content_filter' | null
  latency: number  // milliseconds
}

interface ClientOptions {
  apiKey: string
  model: string
  baseUrl?: string
  defaultParams?: GenerationParams
  referer?: string  // OpenRouter HTTP-Referer header
  title?: string    // OpenRouter X-Title header
}

interface GenerationParams {
  temperature?: number    // 0-2
  maxTokens?: number
  topP?: number          // 0-1
  stop?: string[]
}

interface ChatOptions extends GenerationParams {
  model?: string
  signal?: AbortSignal
  retry?: boolean
  maxRetries?: number
}

interface ConversationOptions {
  system?: string
  initialMessages?: Message[]
}

interface ModelInfo {
  contextLength: number
  pricing: { prompt: number, completion: number }  // per 1M tokens
}
```

---

## 1. Client Creation

### `createLLMClient(options)`

```
âœ“ creates client with apiKey and model
  - Input: { apiKey: 'sk-test', model: 'anthropic/claude-sonnet-4' }
  - Returns: LLMClient instance with chat, stream, createConversation methods

âœ“ throws ValidationError if apiKey is missing
  - Input: { model: 'anthropic/claude-sonnet-4' }
  - Throws: ValidationError with message containing 'apiKey'

âœ“ throws ValidationError if apiKey is empty string
  - Input: { apiKey: '', model: 'anthropic/claude-sonnet-4' }
  - Throws: ValidationError

âœ“ throws ValidationError if model is missing
  - Input: { apiKey: 'sk-test' }
  - Throws: ValidationError with message containing 'model'

âœ“ throws ValidationError if model is empty string
  - Input: { apiKey: 'sk-test', model: '' }
  - Throws: ValidationError

âœ“ accepts custom baseUrl
  - Input: { apiKey: 'sk-test', model: 'gpt-4', baseUrl: 'https://api.openai.com/v1' }
  - Client uses provided baseUrl for requests

âœ“ defaults baseUrl to OpenRouter
  - Input: { apiKey: 'sk-test', model: 'anthropic/claude-sonnet-4' }
  - Requests go to 'https://openrouter.ai/api/v1'

âœ“ accepts defaultParams for generation settings
  - Input: { apiKey: 'sk-test', model: 'x', defaultParams: { temperature: 0.7, maxTokens: 1000 } }
  - These params are used in subsequent chat() calls

âœ“ accepts referer option for OpenRouter header
  - Input: { apiKey: 'sk-test', model: 'x', referer: 'https://myapp.com' }
  - HTTP-Referer header uses provided value

âœ“ accepts title option for OpenRouter header
  - Input: { apiKey: 'sk-test', model: 'x', title: 'My Application' }
  - X-Title header uses provided value

âœ“ does not make any network requests on creation
  - Creating client should not trigger any fetch calls
  - Verified by checking mock fetch was not called
```

---

## 2. Basic Chat Completion

### `client.chat(messages)`

```
âœ“ sends messages array to API and returns response
  - Input: [{ role: 'user', content: 'Hello' }]
  - Makes POST request to /chat/completions
  - Returns ChatResponse object

âœ“ returns response with content string
  - Response: { content: 'Hello! How can I help?' }
  - content is extracted from choices[0].message.content

âœ“ returns response with usage stats
  - Response: { usage: { promptTokens: 10, completionTokens: 20, totalTokens: 30 } }
  - Maps from API's prompt_tokens/completion_tokens/total_tokens

âœ“ returns response with model identifier
  - Response: { model: 'anthropic/claude-sonnet-4' }
  - model reflects actual model used (may differ from requested)

âœ“ returns response with request id
  - Response: { id: 'chatcmpl-abc123' }
  - id is from API response

âœ“ returns response with finishReason
  - Response: { finishReason: 'stop' }
  - Maps from choices[0].finish_reason
  - Possible values: 'stop', 'length', 'content_filter', null

âœ“ returns response with latency in milliseconds
  - Response: { latency: 1234 }
  - Measured from request start to response complete

âœ“ handles single user message
  - Input: [{ role: 'user', content: 'Hi' }]
  - Sends array with one message

âœ“ handles system + user messages
  - Input: [{ role: 'system', content: 'Be helpful' }, { role: 'user', content: 'Hi' }]
  - Sends both messages in order

âœ“ handles full conversation history
  - Input: [system, user, assistant, user, assistant, user]
  - Sends complete history to API

âœ“ handles response with null usage data
  - API returns: { choices: [...], usage: null }
  - Response: { usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 } }

âœ“ handles response with missing usage field
  - API returns: { choices: [...] } (no usage key)
  - Response: { usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 } }
```

### Request Format

```
âœ“ sends Authorization header with Bearer token
  - Header: 'Authorization: Bearer sk-test'

âœ“ sends Content-Type header as application/json
  - Header: 'Content-Type: application/json'

âœ“ sends model in request body
  - Body contains: { model: 'anthropic/claude-sonnet-4' }

âœ“ sends messages array in request body
  - Body contains: { messages: [...] }
  - Each message has role and content fields

âœ“ sends HTTP-Referer header for OpenRouter (default baseUrl)
  - Header: 'HTTP-Referer: https://localhost' (default)
  - Or custom value if referer option provided

âœ“ sends X-Title header for OpenRouter (default baseUrl)
  - Header: 'X-Title: @motioneffector/llm' (default)
  - Or custom value if title option provided

âœ“ omits OpenRouter headers when baseUrl is not OpenRouter
  - baseUrl: 'https://api.openai.com/v1'
  - HTTP-Referer and X-Title headers are NOT sent
  - Detection: baseUrl does not contain 'openrouter'

âœ“ sends stream: false for non-streaming requests
  - Body contains: { stream: false }
```

---

## 3. Message Validation

### Input Validation

```
âœ“ throws ValidationError for empty messages array
  - Input: []
  - Throws: ValidationError with message 'messages array cannot be empty'
  - No network request is made

âœ“ throws ValidationError for invalid role
  - Input: [{ role: 'admin', content: 'Hi' }]
  - Throws: ValidationError with message containing 'role'
  - Valid roles: 'system', 'user', 'assistant'

âœ“ throws TypeError for null content
  - Input: [{ role: 'user', content: null }]
  - Throws: TypeError with message containing 'content'

âœ“ throws TypeError for undefined content
  - Input: [{ role: 'user', content: undefined }]
  - Throws: TypeError

âœ“ throws TypeError for non-string content
  - Input: [{ role: 'user', content: 123 }]
  - Throws: TypeError

âœ“ allows empty string content
  - Input: [{ role: 'user', content: '' }]
  - Does NOT throw, sends to API

âœ“ allows whitespace-only content
  - Input: [{ role: 'user', content: '   ' }]
  - Does NOT throw, sends to API

âœ“ validates all messages in array, not just first
  - Input: [{ role: 'user', content: 'Hi' }, { role: 'invalid', content: 'x' }]
  - Throws: ValidationError for second message

âœ“ handles messages with unicode and emoji
  - Input: [{ role: 'user', content: 'ä½ å¥½ ðŸ‘‹ Ù…Ø±Ø­Ø¨Ø§' }]
  - Sends correctly encoded

âœ“ handles messages with newlines
  - Input: [{ role: 'user', content: 'Line 1\nLine 2\n\nLine 4' }]
  - Sends with newlines preserved

âœ“ does not validate message length (let API handle)
  - Very long content is sent to API
  - API may return error for exceeding context
```

---

## 4. Chat Options

### `client.chat(messages, options)`

```
âœ“ temperature option is sent to API
  - Options: { temperature: 0.5 }
  - Body contains: { temperature: 0.5 }

âœ“ temperature must be between 0 and 2
  - Options: { temperature: 2.5 }
  - Throws: ValidationError

âœ“ maxTokens option is sent as max_tokens
  - Options: { maxTokens: 500 }
  - Body contains: { max_tokens: 500 }

âœ“ topP option is sent as top_p
  - Options: { topP: 0.9 }
  - Body contains: { top_p: 0.9 }

âœ“ model option overrides default model
  - Client default: 'anthropic/claude-sonnet-4'
  - Options: { model: 'openai/gpt-4' }
  - Body contains: { model: 'openai/gpt-4' }

âœ“ stop option sends stop sequences
  - Options: { stop: ['END', '###'] }
  - Body contains: { stop: ['END', '###'] }

âœ“ options override defaultParams from client creation
  - Client defaultParams: { temperature: 0.7 }
  - Options: { temperature: 0.2 }
  - Body contains: { temperature: 0.2 }

âœ“ options merge with defaultParams (non-conflicting)
  - Client defaultParams: { temperature: 0.7 }
  - Options: { maxTokens: 100 }
  - Body contains: { temperature: 0.7, max_tokens: 100 }

âœ“ undefined options do not override defaultParams
  - Client defaultParams: { temperature: 0.7 }
  - Options: { temperature: undefined }
  - Body contains: { temperature: 0.7 }
```

---

## 5. Streaming

### `client.stream(messages)`

```
âœ“ returns async iterable
  - const stream = client.stream(messages)
  - stream[Symbol.asyncIterator] exists

âœ“ yields string chunks as they arrive
  - for await (const chunk of stream) { /* chunk is string */ }

âœ“ final concatenation equals complete response
  - let full = ''; for await (const c of stream) full += c;
  - full === complete assistant response

âœ“ sends stream: true in request body
  - Body contains: { stream: true }

âœ“ handles SSE format correctly
  - Parses 'data: {...}\n\n' format

âœ“ handles [DONE] signal
  - 'data: [DONE]\n\n' terminates stream
  - No error thrown, iteration ends

âœ“ handles data: prefix in SSE lines
  - Strips 'data: ' prefix before JSON parsing

âœ“ handles multiple chunks in single SSE event
  - Some APIs batch multiple deltas

âœ“ skips empty SSE lines
  - Blank lines between events are ignored

âœ“ skips SSE comments (lines starting with :)
  - ': keep-alive' lines are ignored
```

### Stream Iteration

```
âœ“ can iterate with for-await-of
  - for await (const chunk of client.stream(msgs)) { }
  - Works correctly

âœ“ can break early from iteration
  - for await (const chunk of stream) { break; }
  - No error thrown, resources cleaned up

âœ“ stream is single-use (cannot iterate twice)
  - const stream = client.stream(msgs)
  - First iteration: works
  - Second iteration: yields nothing or throws

âœ“ partially consumed stream cleans up automatically
  - No explicit dispose() needed after break
```

### Stream Options

```
âœ“ accepts same options as chat()
  - client.stream(messages, { temperature: 0.5 })
  - Options sent in request body

âœ“ accepts AbortSignal
  - client.stream(messages, { signal })
  - Stream can be cancelled mid-iteration
```

### Stream Edge Cases

```
âœ“ handles empty stream (no content chunks)
  - API sends: 'data: [DONE]' immediately
  - Full response is empty string ''
  - No error thrown

âœ“ handles connection drop mid-stream
  - Network fails during iteration
  - Throws: NetworkError

âœ“ handles malformed SSE chunk
  - 'data: {invalid json}\n\n'
  - Throws: ParseError

âœ“ handles chunk with empty content delta
  - delta: { content: '' }
  - Skipped, does not yield empty string
```

---

## 6. Conversation Management

### `client.createConversation(options?)`

```
âœ“ creates conversation object
  - Returns object with send, sendStream, history, clear methods

âœ“ accepts optional system prompt
  - Options: { system: 'You are helpful' }
  - System message prepended to all requests

âœ“ accepts optional initial messages
  - Options: { initialMessages: [{ role: 'user', content: 'Hi' }] }
  - History starts with these messages

âœ“ allows system message in initialMessages
  - Options: { initialMessages: [{ role: 'system', content: 'Be brief' }] }
  - Valid, used as system prompt

âœ“ allows both system and initialMessages with system
  - Options: { system: 'X', initialMessages: [{ role: 'system', content: 'Y' }] }
  - system option takes precedence, initialMessages system is kept in history

âœ“ allows empty initialMessages array
  - Options: { initialMessages: [] }
  - Equivalent to not providing it

âœ“ allows non-alternating messages in initialMessages
  - Options: { initialMessages: [user, user, assistant] }
  - Valid, no alternation requirement

âœ“ starts with empty history if no options
  - const conv = client.createConversation()
  - conv.history returns []
```

### `conversation.send(content)`

```
âœ“ sends user message and returns assistant response string
  - Input: 'Hello'
  - Returns: 'Hi there! How can I help?'

âœ“ adds user message to history before request
  - Input: 'Hello'
  - history includes { role: 'user', content: 'Hello' }

âœ“ adds assistant response to history after completion
  - Response: 'Hi there!'
  - history includes { role: 'assistant', content: 'Hi there!' }

âœ“ subsequent send() includes full history
  - send('Hello') â†’ 'Hi'
  - send('How are you?')
  - Second request includes: [user: Hello, assistant: Hi, user: How are you?]

âœ“ system prompt is included first in every request
  - system: 'Be helpful'
  - Every API request starts with system message

âœ“ throws ConcurrencyError if called while previous send is pending
  - send('First') // don't await
  - send('Second') // throws ConcurrencyError immediately
```

### `conversation.sendStream(content)`

```
âœ“ sends user message and returns async iterable
  - Input: 'Hello'
  - Returns: AsyncIterable<string>

âœ“ adds user message to history immediately (before streaming)
  - sendStream('Hello')
  - history immediately includes user message

âœ“ adds complete assistant response to history after stream ends
  - Stream yields: 'Hi', ' there'
  - After iteration: history includes { role: 'assistant', content: 'Hi there' }

âœ“ does NOT add partial response to history if stream errors
  - Stream yields 'Hi' then throws NetworkError
  - history does NOT include partial assistant message
  - User message IS in history (was added before request)

âœ“ does NOT add response to history if stream is aborted
  - Stream cancelled via AbortSignal
  - history does NOT include assistant message

âœ“ throws ConcurrencyError if called while previous request is pending
  - sendStream('First') // don't await iteration
  - sendStream('Second') // throws ConcurrencyError
```

### `conversation.history`

```
âœ“ returns full message history array
  - Returns: Message[]

âœ“ includes system message if set (as first element)
  - system: 'Be helpful'
  - history[0] === { role: 'system', content: 'Be helpful' }

âœ“ returns defensive copy (mutations don't affect internal state)
  - const h = conversation.history
  - h.push({ role: 'user', content: 'X' })
  - conversation.history does NOT include X

âœ“ returns empty array for new conversation without initialMessages
  - const conv = client.createConversation()
  - conv.history.length === 0
```

### `conversation.addMessage(role, content)`

```
âœ“ manually adds message to history
  - addMessage('user', 'Injected')
  - history includes the message

âœ“ accepts 'user' role
  - addMessage('user', 'Hello')
  - Works correctly

âœ“ accepts 'assistant' role
  - addMessage('assistant', 'Hi')
  - Works correctly

âœ“ throws ValidationError for 'system' role
  - addMessage('system', 'X')
  - Throws: ValidationError with message 'use constructor for system prompt'

âœ“ throws ValidationError for invalid role
  - addMessage('admin', 'X')
  - Throws: ValidationError

âœ“ throws TypeError for non-string content
  - addMessage('user', null)
  - Throws: TypeError

âœ“ throws ConcurrencyError if called during pending request
  - send('Hello') // don't await
  - addMessage('user', 'X') // throws
```

### `conversation.clear()`

```
âœ“ clears all messages except system prompt
  - After multiple send() calls
  - clear()
  - history only contains system message (if set)

âœ“ system prompt retained if originally set
  - system: 'Be helpful'
  - send() then clear()
  - history === [{ role: 'system', content: 'Be helpful' }]

âœ“ results in empty history if no system prompt
  - No system option
  - send() then clear()
  - history === []

âœ“ throws ConcurrencyError if called during pending request
  - send('Hello') // don't await
  - clear() // throws
```

### `conversation.clearAll()`

```
âœ“ clears all messages including system prompt
  - system: 'Be helpful'
  - send() then clearAll()
  - history === []

âœ“ throws ConcurrencyError if called during pending request
  - send('Hello') // don't await
  - clearAll() // throws
```

---

## 7. Request Cancellation

### `client.chat(messages, { signal })`

```
âœ“ accepts AbortSignal option
  - const controller = new AbortController()
  - client.chat(messages, { signal: controller.signal })

âœ“ aborts in-flight request when signal fires
  - controller.abort() during request
  - Underlying fetch is aborted

âœ“ throws AbortError when cancelled
  - controller.abort()
  - Throws: AbortError (or DOMException with name 'AbortError')

âœ“ throws AbortError immediately for pre-aborted signal
  - const controller = new AbortController()
  - controller.abort()
  - client.chat(messages, { signal: controller.signal })
  - Throws immediately, no request made

âœ“ AbortError includes abort reason if provided
  - controller.abort(new Error('User cancelled'))
  - error.cause === abort reason
```

### `client.stream(messages, { signal })`

```
âœ“ accepts AbortSignal option
  - const controller = new AbortController()
  - client.stream(messages, { signal: controller.signal })

âœ“ stops stream iteration when signal fires
  - controller.abort() during for-await
  - Iteration terminates

âœ“ throws AbortError when stream is aborted
  - Throws during next iteration after abort

âœ“ partially yielded content is available before abort
  - Stream yields 'Hello', ' world'
  - Abort after 'Hello'
  - Already yielded 'Hello' was received

âœ“ throws immediately for pre-aborted signal
  - Pre-aborted signal passed
  - Throws AbortError before first yield
```

### Abort During Retry

```
âœ“ aborts retry wait when signal fires
  - Request fails with 429
  - During backoff wait, signal.abort()
  - Throws AbortError, does not continue retry
```

---

## 8. Error Handling

### Error Types

```
All custom errors extend Error and have:
- name: string (error class name)
- message: string (human readable)
- cause?: Error (original error if wrapped)

âœ“ ValidationError for input validation failures
âœ“ TypeError for type mismatches
âœ“ RateLimitError for 429 responses
âœ“ AuthError for 401/403 responses
âœ“ ModelError for model-related 404 responses
âœ“ ServerError for 5xx responses
âœ“ NetworkError for fetch/connection failures
âœ“ ParseError for JSON/response parsing failures
âœ“ AbortError for cancelled requests
âœ“ ConcurrencyError for concurrent conversation operations
```

### HTTP Errors

```
âœ“ throws RateLimitError on 429
  - Status: 429
  - Throws: RateLimitError
  - error.status === 429

âœ“ RateLimitError includes retryAfter if header present
  - Response header: 'Retry-After: 30'
  - error.retryAfter === 30

âœ“ throws AuthError on 401
  - Status: 401
  - Throws: AuthError
  - error.status === 401

âœ“ throws AuthError on 403
  - Status: 403
  - Throws: AuthError

âœ“ throws ModelError on 404 (model not found)
  - Status: 404
  - Throws: ModelError
  - error.status === 404

âœ“ throws ServerError on 500
  - Status: 500
  - Throws: ServerError

âœ“ throws ServerError on 502, 503, 504
  - Each status throws ServerError
  - error.status reflects actual status

âœ“ error includes response body message if available
  - Response body: { error: { message: 'Model not found' } }
  - error.message includes 'Model not found'

âœ“ error handles non-JSON error response body
  - Response body: 'Internal server error'
  - error.message includes response text

âœ“ throws ServerError for unknown 4xx/5xx status
  - Status: 418, 599
  - Throws: ServerError with status

âœ“ throws NetworkError for non-HTTP errors during fetch
  - fetch throws TypeError (invalid URL, etc.)
  - Throws: NetworkError with cause
```

### Network Errors

```
âœ“ throws NetworkError on fetch failure (no response)
  - fetch() rejects (network down, DNS failure)
  - Throws: NetworkError

âœ“ throws NetworkError on connection timeout
  - Request times out
  - Throws: NetworkError

âœ“ error.cause contains original error
  - Original: TypeError('Failed to fetch')
  - error.cause === original error
```

### Parse Errors

```
âœ“ throws ParseError on invalid JSON response
  - Response body: 'not json'
  - Status: 200
  - Throws: ParseError

âœ“ throws ParseError on unexpected response structure
  - Response: { unexpected: 'format' }
  - Missing choices array
  - Throws: ParseError

âœ“ throws ParseError on missing content in response
  - Response: { choices: [{ message: {} }] }
  - Throws: ParseError (or returns empty string - decide based on API behavior)

âœ“ ParseError includes response body in message for debugging
  - error.message includes truncated response body
```

---

## 9. Retry Logic

### Automatic Retry

```
âœ“ retries on 429 (rate limit)
  - First request: 429
  - Second request: 200
  - Returns successful response

âœ“ retries on 500
  - Server error, then success
  - Returns successful response

âœ“ retries on 502, 503, 504
  - Each gateway error triggers retry

âœ“ does NOT retry on 400
  - Status: 400
  - Throws immediately, no retry

âœ“ does NOT retry on 401
  - Auth error
  - Throws immediately

âœ“ does NOT retry on 403
  - Forbidden
  - Throws immediately

âœ“ does NOT retry on 404
  - Model not found
  - Throws immediately

âœ“ respects Retry-After header if present
  - Response header: 'Retry-After: 2'
  - Waits at least 2 seconds before retry

âœ“ uses exponential backoff: 1s, 2s, 4s
  - First retry after ~1000ms
  - Second retry after ~2000ms
  - Third retry after ~4000ms
  - (with jitter)

âœ“ backoff capped at 30 seconds
  - Even with many retries, never waits more than 30s

âœ“ maximum 3 retries by default (4 total attempts)
  - Attempts: original + 3 retries
  - After 4th failure, throws error

âœ“ maxRetries option overrides default
  - Options: { maxRetries: 5 }
  - Allows up to 5 retries (6 total attempts)

âœ“ maxRetries: 0 means no retries
  - Options: { maxRetries: 0 }
  - Single attempt, throws on first failure
```

### `client.chat(messages, { retry: false })`

```
âœ“ disables automatic retry entirely
  - Options: { retry: false }
  - 429 response
  - Throws immediately

âœ“ throws immediately on retriable error
  - 503 with retry: false
  - Throws ServerError, no retry
```

### Retry and Streaming

```
âœ“ streaming does NOT retry (fails fast)
  - client.stream(messages)
  - 429 response
  - Throws immediately, no retry

âœ“ streaming retry: true option is ignored
  - Options have no effect for streams
```

---

## 10. Token Estimation

### `estimateTokens(text)`

```
âœ“ returns estimated token count for string
  - Input: 'Hello world'
  - Returns: number (approximately 3)

âœ“ returns 0 for empty string
  - Input: ''
  - Returns: 0

âœ“ uses simple heuristic: characters / 4
  - Input: 'Hello' (5 chars)
  - Returns: approximately 1-2

âœ“ handles unicode correctly
  - Input: 'ä½ å¥½ä¸–ç•Œ' (4 chars, but multi-byte)
  - Returns: reasonable estimate

âœ“ rounds to nearest integer
  - Never returns fractional tokens
```

### `client.estimateChat(messages)`

```
âœ“ estimates tokens for full message array
  - Input: [{ role: 'user', content: 'Hello' }]
  - Returns: { prompt: number }

âœ“ accounts for message structure overhead
  - Each message has ~4 token overhead for role/formatting
  - [{ role: 'user', content: 'Hi' }] > estimateTokens('Hi')

âœ“ sums all message contents
  - Multiple messages
  - Total includes all content

âœ“ returns available tokens based on model context
  - Returns: { prompt: 100, available: 3900 }
  - available = model context limit - prompt estimate

âœ“ uses default context limit if model unknown
  - Unknown model ID
  - Uses conservative default (e.g., 4096)
```

---

## 11. Model Information

### `client.getModel()`

```
âœ“ returns current default model
  - Client created with model: 'anthropic/claude-sonnet-4'
  - Returns: 'anthropic/claude-sonnet-4'
```

### `client.setModel(model)`

```
âœ“ changes default model
  - setModel('openai/gpt-4')
  - getModel() === 'openai/gpt-4'

âœ“ affects subsequent requests
  - setModel('openai/gpt-4')
  - chat() uses 'openai/gpt-4'

âœ“ throws ValidationError for empty model
  - setModel('')
  - Throws: ValidationError

âœ“ does not validate model exists (API will reject invalid)
  - setModel('nonexistent/model')
  - Does not throw (validation happens at API call)
```

### `getModelInfo(modelId)`

Standalone function, not client method.

```
âœ“ returns model context length
  - getModelInfo('anthropic/claude-sonnet-4')
  - Returns: { contextLength: 200000, ... }

âœ“ returns model pricing info
  - Returns: { pricing: { prompt: 3, completion: 15 } }
  - Prices per 1 million tokens

âœ“ returns undefined for unknown model
  - getModelInfo('unknown/model')
  - Returns: undefined

âœ“ includes common models
  - anthropic/claude-sonnet-4
  - anthropic/claude-3-opus
  - openai/gpt-4o
  - openai/gpt-4-turbo
  - meta-llama/llama-3.1-405b
```

---

## 12. Multiple Providers

### OpenRouter Specifics

```
âœ“ handles OpenRouter-specific response format
  - OpenRouter may include additional fields
  - Parses correctly without error

âœ“ includes required OpenRouter headers (default baseUrl)
  - HTTP-Referer header present
  - X-Title header present

âœ“ parses OpenRouter error messages
  - OpenRouter error format: { error: { message, code } }
  - Error message extracted correctly
```

### Base URL Override

```
âœ“ custom baseUrl sends requests to that endpoint
  - baseUrl: 'https://api.openai.com/v1'
  - Requests go to 'https://api.openai.com/v1/chat/completions'

âœ“ works with OpenAI-compatible APIs
  - baseUrl: 'http://localhost:11434/v1' (Ollama)
  - Standard request format works

âœ“ omits OpenRouter-specific headers for non-OpenRouter URLs
  - baseUrl does not contain 'openrouter'
  - HTTP-Referer NOT sent
  - X-Title NOT sent

âœ“ includes OpenRouter headers if URL contains 'openrouter'
  - baseUrl: 'https://custom.openrouter.ai/v1'
  - Headers ARE sent

âœ“ appends /chat/completions to baseUrl
  - baseUrl: 'https://api.example.com/v1'
  - Requests to: 'https://api.example.com/v1/chat/completions'

âœ“ handles baseUrl with trailing slash
  - baseUrl: 'https://api.example.com/v1/'
  - Does not double-slash: '.../v1/chat/completions'
```

---

## 13. Edge Cases

### Request Edge Cases

```
âœ“ handles rapid successive requests
  - await chat(), await chat(), await chat()
  - All complete successfully

âœ“ handles concurrent requests (different calls)
  - Promise.all([chat(a), chat(b), chat(c)])
  - All complete independently

âœ“ handles very long messages (sends to API, may fail)
  - Content: 'x'.repeat(1000000)
  - Request sent, API may return error

âœ“ handles special characters in content
  - Content with quotes, backslashes, etc.
  - JSON serialization handles correctly
```

### Response Edge Cases

```
âœ“ handles response with empty content
  - API returns: { choices: [{ message: { content: '' } }] }
  - Returns: { content: '' }

âœ“ handles response with null content
  - API returns: { choices: [{ message: { content: null } }] }
  - Returns: { content: '' } (coerce to empty string)

âœ“ handles unexpected extra fields in response
  - API returns additional fields
  - Ignores them, extracts known fields
```

### Conversation Edge Cases

```
âœ“ conversation works after clear()
  - send(), clear(), send()
  - Second send works correctly

âœ“ conversation works after error
  - send() throws NetworkError
  - Retry send() works

âœ“ history not corrupted after failed request
  - send('Hello') throws
  - history still valid (may include user message)
```

---

## 14. Exports

### Module Exports

```
âœ“ exports createLLMClient function
  - import { createLLMClient } from '@motioneffector/llm'

âœ“ exports estimateTokens function
  - import { estimateTokens } from '@motioneffector/llm'

âœ“ exports getModelInfo function
  - import { getModelInfo } from '@motioneffector/llm'

âœ“ exports error classes
  - import { ValidationError, RateLimitError, AuthError, ... } from '@motioneffector/llm'

âœ“ exports TypeScript types
  - import type { Message, ChatResponse, TokenUsage, ... } from '@motioneffector/llm'
```

---

## Test Utilities Needed

```
Mock Infrastructure:
- Mock fetch implementation that can return configured responses
- Mock SSE stream generator for streaming tests
- Fixture responses for success, errors, and edge cases
- Helper to create test client with mocked fetch

Timing Utilities:
- Fake timers for retry backoff testing
- Latency measurement verification

Assertion Helpers:
- Assert request body contains expected fields
- Assert headers are correct
- Assert error type and properties
```

---

## Test Organization

Suggested test file structure:

```
tests/
â”œâ”€â”€ client.test.ts          # createLLMClient, basic options
â”œâ”€â”€ chat.test.ts            # client.chat() functionality
â”œâ”€â”€ stream.test.ts          # client.stream() functionality
â”œâ”€â”€ conversation.test.ts    # Conversation management
â”œâ”€â”€ validation.test.ts      # Input validation, error types
â”œâ”€â”€ retry.test.ts           # Retry logic, backoff
â”œâ”€â”€ cancellation.test.ts    # AbortSignal handling
â”œâ”€â”€ tokens.test.ts          # Token estimation
â”œâ”€â”€ models.test.ts          # Model info, switching
â”œâ”€â”€ providers.test.ts       # Multi-provider, baseUrl
â””â”€â”€ fixtures/
    â”œâ”€â”€ responses.ts        # Mock API responses
    â””â”€â”€ streams.ts          # Mock SSE streams
```
